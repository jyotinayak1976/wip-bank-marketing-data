{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90a38015-ac18-43c7-af0b-eabf754be37b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 516) (2861340117.py, line 516)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 516\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint('\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 516)\n"
     ]
    }
   ],
   "source": [
    "# Bank Marketing - Full EDA Notebook\n",
    "# ---------------------------------\n",
    "# This script is written as a Jupyter-style notebook but saved as a single Python file.\n",
    "# It performs a full Exploratory Data Analysis for the UCI Bank Marketing dataset.\n",
    "# How to use:\n",
    "# - Place this file in your project (e.g., in notebooks/)\n",
    "# - Make sure PROJECT_ROOT is set or automatically discovered (see detection section)\n",
    "# - Run it in a Jupyter environment or convert to a .ipynb using jupytext if desired\n",
    "\n",
    "# %%\n",
    "\"\"\"## 0. Imports and config\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "# %%\n",
    "\"\"\"## 1. Project root detection and paths\n",
    "We attempt automatic detection first; fallback to a hard-coded path if needed.\n",
    "Replace the hard-coded path below with your project root if auto-detection fails.\n",
    "\"\"\"\n",
    "# Option A: automatic detection (works if this file is in <root>/notebooks or similar)\n",
    "try:\n",
    "    PROJECT_ROOT = Path().resolve()\n",
    "    # if running from notebooks folder, assume parent is project root\n",
    "    if PROJECT_ROOT.name.lower() in (\"notebooks\", \"notebook\"):\n",
    "        PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "except Exception:\n",
    "    PROJECT_ROOT = None\n",
    "\n",
    "# Option B: fallback (uncomment and edit if auto-detect is not correct)\n",
    "if PROJECT_ROOT is None or not (PROJECT_ROOT.exists()):\n",
    "    PROJECT_ROOT = Path(r\"D:\\development\\ML Dev projects\\Bank-Marketing-ML-Dev\")\n",
    "\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\" / \"figures\"\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_RAW:\", DATA_RAW)\n",
    "print(\"REPORTS_DIR:\", REPORTS_DIR)\n",
    "\n",
    "# %%\n",
    "\"\"\"## 2. Load the data\n",
    "The UCI Bank Marketing dataset uses semicolons as separators and double quotes for quoted fields.\n",
    "\"\"\"\n",
    "# Replace the filename if different\n",
    "csv_path = DATA_RAW / \"bank-full.csv\"\n",
    "if not csv_path.exists():\n",
    "    # try common alternative names\n",
    "    for alt in [\"bank.csv\", \"bank_marketing.csv\", \"bank-full.csv\"]:\n",
    "        p = DATA_RAW / alt\n",
    "        if p.exists():\n",
    "            csv_path = p\n",
    "            break\n",
    "\n",
    "print('Reading:', csv_path)\n",
    "\n",
    "# Read using pandas with proper separator and quoting\n",
    "data = pd.read_csv(csv_path, sep=';', quotechar='\"')\n",
    "\n",
    "# Quick info\n",
    "print('\\nData shape:', data.shape)\n",
    "print('\\nColumns:\\n', data.columns.tolist())\n",
    "\n",
    "# %%\n",
    "\"\"\"## 3. Quick data sanity checks\"\"\"\n",
    "# Show first rows\n",
    "display(data.head())\n",
    "\n",
    "# dtypes and missing values\n",
    "print('\\nData types:')\n",
    "print(data.dtypes)\n",
    "\n",
    "print('\\nMissing values per column:')\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Unique counts for each column\n",
    "print('\\nUnique value counts:')\n",
    "print(data.nunique())\n",
    "\n",
    "# %%\n",
    "\"\"\"## 4. Column typing (numerical / categorical / binary)\n",
    "Re-using your earlier logic: a column is categorical if dtype is object/categorical or nunique < 10.\n",
    "\"\"\"\n",
    "from pandas.api.types import is_numeric_dtype, is_object_dtype, is_categorical_dtype\n",
    "\n",
    "cat_threshold = 10\n",
    "categorical_cols = []\n",
    "numerical_cols = []\n",
    "binary_cols = []\n",
    "\n",
    "for col in data.columns:\n",
    "    unique_vals = data[col].nunique()\n",
    "    if unique_vals == 2:\n",
    "        binary_cols.append(col)\n",
    "        continue\n",
    "    if is_object_dtype(data[col]) or is_categorical_dtype(data[col]) or unique_vals < cat_threshold:\n",
    "        categorical_cols.append(col)\n",
    "        continue\n",
    "    if is_numeric_dtype(data[col]):\n",
    "        numerical_cols.append(col)\n",
    "\n",
    "print('Categorical:', categorical_cols)\n",
    "print('Numerical:', numerical_cols)\n",
    "print('Binary   :', binary_cols)\n",
    "\n",
    "# %%\n",
    "\"\"\"## 5. Target distribution (y)\n",
    "Check class balance and save a plot.\"\"\"\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=data, x='y')\n",
    "plt.title('Target distribution (y)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'target_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "print('\\nProportions:')\n",
    "print(data['y'].value_counts(normalize=True))\n",
    "\n",
    "# %%\n",
    "\"\"\"## 6. Numerical variable exploration\n",
    "- Distribution (histogram + KDE)\n",
    "- Boxplot split by target\n",
    "\"\"\"\n",
    "for col in numerical_cols:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10,3.5))\n",
    "    sns.histplot(data=data, x=col, kde=True, ax=axes[0])\n",
    "    axes[0].set_title(f'Distribution of {col}')\n",
    "\n",
    "    sns.boxplot(data=data, x='y', y=col, ax=axes[1])\n",
    "    axes[1].set_title(f'{col} by target (y)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fname = REPORTS_DIR / f\"num_{col}.png\"\n",
    "    plt.savefig(fname)\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "\"\"\"## 7. Categorical variable exploration\n",
    "- Countplots split by target\n",
    "- Conversion rates per category\n",
    "\"\"\"\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.countplot(data=data, x=col, hue='y')\n",
    "    plt.title(f'{col} counts by target')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / f\"cat_{col}_counts.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # conversion rates\n",
    "    conv = data.groupby(col)['y'].apply(lambda x: (x==\"yes\").mean()).sort_values(ascending=False)\n",
    "    print(f\"\\nConversion rate by {col}:\\n\", conv)\n",
    "\n",
    "# %%\n",
    "\"\"\"## 8. Binary features effect on conversion\n",
    "Plot mean(y==yes) for each binary feature.\"\"\"\n",
    "for col in binary_cols:\n",
    "    if col == 'y':\n",
    "        continue\n",
    "    plt.figure(figsize=(6,4))\n",
    "    rate = data.groupby(col)['y'].apply(lambda x: (x==\"yes\").mean())\n",
    "    rate = rate.reset_index()\n",
    "    sns.barplot(data=rate, x=col, y='y')\n",
    "    plt.ylabel('Conversion rate (mean of y==yes)')\n",
    "    plt.title(f'Conversion rate by {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / f\"bin_{col}_conversion.png\")\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "\"\"\"## 9. Correlation analysis for numerical columns\n",
    "Correlation matrix + heatmap. Note: 'y' should be numeric for correlation; encode temporarily.\n",
    "\"\"\"\n",
    "corr_df = data[numerical_cols].copy()\n",
    "# temporary numeric encoding of y\n",
    "corr_with_y = data[numerical_cols].assign(y_numeric=data['y'].map({'no':0,'yes':1}))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr_with_y.corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation matrix (numerical + y)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'corr_heatmap.png')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "\"\"\"## 10. Feature engineering suggestions and notes\n",
    "- `duration` is highly predictive but *leaks* because it is available only after contact; exclude for live prediction.\n",
    "- `balance` is skewed: consider log1p transform for algorithms sensitive to distribution.\n",
    "- `campaign` (number of contacts) shows diminishing returns: consider bucketing.\n",
    "- `pdays` uses 999 to indicate not previously contacted; create an indicator `pdays_not_contacted`.\n",
    "\n",
    "Let's create these candidate engineered features (for EDA only).\n",
    "\"\"\"\n",
    "# create engineered features for analysis\n",
    "edata = data.copy()\n",
    "# encode pdays_not_contacted\n",
    "if 'pdays' in edata.columns:\n",
    "    edata['pdays_not_contacted'] = (edata['pdays'] == 999).astype(int)\n",
    "\n",
    "# log transform balance for visualization\n",
    "edata['balance_log1p'] = np.log1p(edata['balance'] - edata['balance'].min()+1)\n",
    "\n",
    "# bucket campaign\n",
    "edata['campaign_bucket'] = pd.cut(edata['campaign'], bins=[-1,0,1,2,4,10,100], labels=['0','1','2','3-4','5-10','10+'])\n",
    "\n",
    "# show quick comparison\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(data=edata, x='y', y='balance_log1p')\n",
    "plt.title('Log-transformed balance by target')\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'balance_log1p_by_y.png')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "\"\"\"## 11. Missing values and rare categories handling\"\"\"\n",
    "print('Missing values per column:\\n', data.isnull().sum())\n",
    "\n",
    "# Rare categories\n",
    "for col in categorical_cols:\n",
    "    vc = data[col].value_counts(normalize=True)\n",
    "    rare = vc[vc < 0.01]\n",
    "    if not rare.empty:\n",
    "        print(f\"\\nColumn {col} has rare categories (<1%):\\n\", rare.index.tolist())\n",
    "\n",
    "# %%\n",
    "\"\"\"## 12. Prepare a quick modeling-ready dataset (for prototyping)\n",
    "- Drop `duration` for real predictive models (but keep for experimentation)\n",
    "- One-hot encode categoricals (drop_first to avoid collinearity for linear models)\n",
    "- Keep a copy with and without duration\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# encode target\n",
    "data_model = data.copy()\n",
    "data_model['y_bin'] = data_model['y'].map({'no':0, 'yes':1})\n",
    "\n",
    "# drop duration for final model candidate\n",
    "X = pd.get_dummies(data_model.drop(columns=['y','y_bin','duration']), columns=categorical_cols+binary_cols, drop_first=True)\n",
    "y = data_model['y_bin']\n",
    "\n",
    "print('\\nModel matrix shape (without duration):', X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "print('Train/test split:', X_train.shape, X_test.shape)\n",
    "\n",
    "# %%\n",
    "\"\"\"## 13. Simple baseline model (Logistic Regression) - for sanity check\n",
    "We will run a quick logistic regression and output classification report.\n",
    "\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "y_prob = lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('\\nClassification report (Logistic Regression):')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('\\nROC AUC:', roc_auc_score(y_test, y_prob))\n",
    "\n",
    "# %%\n",
    "\"\"\"## 14. Save a short EDA report (CSV summary tables)\"\"\"\n",
    "# conversion rates per categorical\n",
    "conv_tables = {}\n",
    "for col in categorical_cols:\n",
    "    conv = data.groupby(col)['y'].apply(lambda x: (x==\"yes\").mean()).sort_values(ascending=False)\n",
    "    conv_tables[col] = conv\n",
    "    conv.to_csv(REPORTS_DIR / f'conv_rate_{col}.csv')\n",
    "\n",
    "# numerical summary\n",
    "data[numerical_cols].describe().to_csv(REPORTS_DIR / 'numerical_summary.csv')\n",
    "\n",
    "print('Saved conversion tables and numerical summary to', REPORTS_DIR)\n",
    "\n",
    "# %%\n",
    "\"\"\"## 15. Next steps (suggested)\n",
    "- Build and compare tree-based models (RandomForest, XGBoost, LightGBM).\n",
    "- Use calibration and probability-based ranking for marketing (lift charts).\n",
    "- Address imbalance with class weights or resampling (SMOTE) and evaluate with precision-recall.\n",
    "- Create a simple FastAPI endpoint to serve model predictions (and exclude `duration` from features).\n",
    "- Add dataset and model versioning using DVC and a small CI pipeline for retraining.\n",
    "\n",
    "You can convert this script into a Jupyter notebook using jupytext, or copy cells into a notebook for interactivity.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "\"\"\"## 16. Profiling Reports (YData & Pandas)\n",
    "This section generates profiling reports using **ydata-profiling** (the modern name) and the legacy **pandas-profiling** import alias.\n",
    "It saves HTML reports into `reports/profiling/` and shows them in-notebook when possible.\n",
    "\"\"\"\n",
    "\n",
    "output_profile_dir = REPORTS_DIR.parent / \"profiling\"\n",
    "output_profile_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- YData profiling ---\n",
    "try:\n",
    "    from ydata_profiling import ProfileReport\n",
    "    profile = ProfileReport(\n",
    "        data,\n",
    "        title=\"Bank Marketing Dataset - YData Profiling Report\",\n",
    "        explorative=True\n",
    "    )\n",
    "    ydata_path = output_profile_dir / \"bank_marketing_ydata_profiling.html\"\n",
    "    profile.to_file(ydata_path)\n",
    "    print('Saved YData profiling report to', ydata_path)\n",
    "except Exception as e:\n",
    "    print('Could not run ydata_profiling:', e)\n",
    "\n",
    "# --- Legacy pandas-profiling (alias) ---\n",
    "try:\n",
    "    from pandas_profiling import ProfileReport as PandasProfile\n",
    "    profile_old = PandasProfile(data, title=\"Bank Marketing Dataset - Pandas Profiling Report\")\n",
    "    pandas_path = output_profile_dir / \"bank_marketing_pandas_profiling.html\"\n",
    "    profile_old.to_file(pandas_path)\n",
    "    print('Saved pandas-profiling report to', pandas_path)\n",
    "except Exception as e:\n",
    "    print('Could not run pandas_profiling:', e)\n",
    "\n",
    "# %%\n",
    "\"\"\"## 17. Automated profiling summary extraction\n",
    "Create compact CSV summaries from the profiling output for quick review in CI or dashboards.\n",
    "We derive: missing values table, top correlations with target, and column type summary.\n",
    "\"\"\"\n",
    "\n",
    "# Missing values summary\n",
    "missing_df = data.isnull().sum().rename('missing_count').to_frame()\n",
    "missing_df['missing_pct'] = missing_df['missing_count'] / len(data)\n",
    "missing_df.to_csv(output_profile_dir / 'missing_summary.csv')\n",
    "\n",
    "# Column types & unique counts\n",
    "col_summary = pd.DataFrame({\n",
    "    'dtype': data.dtypes.astype(str),\n",
    "    'nunique': data.nunique(),\n",
    "})\n",
    "col_summary.to_csv(output_profile_dir / 'columns_summary.csv')\n",
    "\n",
    "# Correlation with target (numeric only)\n",
    "if 'y' in data.columns:\n",
    "    numeric_for_corr = data.select_dtypes(include=[np.number]).copy()\n",
    "    numeric_for_corr['y_num'] = data['y'].map({'no':0,'yes':1})\n",
    "    corr_with_target = numeric_for_corr.corr()['y_num'].sort_values(ascending=False)\n",
    "    corr_with_target.to_csv(output_profile_dir / 'corr_with_target.csv')\n",
    "\n",
    "print('Saved automated profiling summaries to', output_profile_dir)\n",
    "\n",
    "# %%\n",
    "\"\"\"## 18. Full ML pipeline: preprocessing, modeling, evaluation, and exports\n",
    "This section builds a reproducible pipeline including preprocessing (with ColumnTransformer), model comparison (logistic, random forest, lightgbm/xgboost if available), grid search, and evaluation.\n",
    "It intentionally excludes `duration` from features for production-ready models.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# 18.1 Prepare modeling dataset\n",
    "model_data = data.copy()\n",
    "model_data['y_bin'] = model_data['y'].map({'no':0,'yes':1})\n",
    "\n",
    "# Drop duration for production model\n",
    "features = [c for c in model_data.columns if c not in ['y','y_bin','duration']]\n",
    "\n",
    "# Recompute categorical/numeric lists (safer to base on dtype)\n",
    "categorical = [c for c in features if model_data[c].dtype == 'object']\n",
    "numeric = [c for c in features if np.issubdtype(model_data[c].dtype, np.number)]\n",
    "\n",
    "print('Model features:', len(features))\n",
    "print('Categorical:', categorical)\n",
    "print('Numeric:', numeric)\n",
    "\n",
    "X = model_data[features]\n",
    "y = model_data['y_bin']\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 18.2 Preprocessing pipeline\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric),\n",
    "    ('cat', cat_pipeline, categorical)\n",
    "], remainder='drop')\n",
    "\n",
    "# 18.3 Models to compare\n",
    "models = {\n",
    "    'logistic': LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    'rf': RandomForestClassifier(n_jobs=-1, class_weight='balanced')\n",
    "}\n",
    "\n",
    "print(\"everything is OK till this point\")\n",
    "# Try to add LightGBM and XGBoost if installed\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    models['lgb'] = lgb.LGBMClassifier(n_jobs=-1)\n",
    "except Exception:\n",
    "    print('LightGBM not available')\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    models['xgb'] = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n",
    "except Exception:\n",
    "    print('XGBoost not available')\n",
    "\n",
    "# 18.4 Helper to fit and evaluate\n",
    "results = {}\n",
    "for name, estimator in models.items():\n",
    "    print('Training pipeline for', name)\n",
    "    pipe = Pipeline([\n",
    "        ('preproc', preprocessor),\n",
    "        ('clf', estimator)\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    if hasattr(pipe, 'predict_proba'):\n",
    "        y_prob = pipe.predict_proba(X_test)[:,1]\n",
    "    else:\n",
    "        # some xgboost wrappers might not have predict_proba in older versions\n",
    "        y_prob = pipe.named_steps['clf'].predict_proba(preprocessor.transform(X_test))[:,1]\n",
    "\n",
    "    print('Model:', name)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('ROC AUC:', roc_auc_score(y_test, y_prob))\n",
    "\n",
    "    # Precision-recall AUC\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print('PR AUC:', pr_auc)\n",
    "\n",
    "    # Save\n",
    "    model_path = PROJECT_ROOT / 'models'\n",
    "    model_path.mkdir(exist_ok=True, parents=True)\n",
    "    joblib.dump(pipe, model_path / f'{name}_pipeline.joblib')\n",
    "    print('Saved pipeline to', model_path / f'{name}_pipeline.joblib')\n",
    "\n",
    "    results[name] = {\n",
    "        'roc_auc': roc_auc_score(y_test, y_prob),\n",
    "        'pr_auc': pr_auc\n",
    "    }\n",
    "\n",
    "print('Model comparison results:', results)\n",
    "\n",
    "# %%\n",
    "\"\"\"## 19. Simple hyperparameter tuning example (RandomForest)\n",
    "A GridSearchCV example with a small parameter grid. For larger searches consider RandomizedSearchCV or Optuna.\n",
    "\"\"\"\n",
    "\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 300],\n",
    "    'clf__max_depth': [None, 6, 12]\n",
    "}\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', RandomForestClassifier(n_jobs=-1, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(rf_pipe, param_grid, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best params:', grid.best_params_)\n",
    "print('Best CV score:', grid.best_score_)\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(grid.best_estimator_, PROJECT_ROOT / 'models' / 'rf_grid_best.joblib')\n",
    "print('Saved best RF grid model')\n",
    "\n",
    "# %%\n",
    "\"\"\"## 20. Model explainability hints\n",
    "- Use SHAP (shap.Explainer) on tree-based models to extract feature importances and local explanations.\n",
    "- For logistic regression, coefficients + odds ratios are useful.\n",
    "\n",
    "# Quick feature importance for the saved RF grid model\n",
    "\"\"\"\n",
    "try:\n",
    "    best = grid.best_estimator_\n",
    "    # Extract feature names after preprocessing\n",
    "    ohe_cols = []\n",
    "    if isinstance(best.named_steps['preproc'].named_transformers_.get('cat').named_steps['onehot'], OneHotEncoder):\n",
    "        ohe = best.named_steps['preproc'].named_transformers_['cat'].named_steps['onehot']\n",
    "        ohe_cols = list(ohe.get_feature_names_out(categorical))\n",
    "\n",
    "    feature_names = numeric + ohe_cols\n",
    "    importances = best.named_steps['clf'].feature_importances_\n",
    "    fi = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "    print('\n",
    "Top features from RF:')\n",
    "    print(fi.head(20))\n",
    "    fi.to_csv(PROJECT_ROOT / 'models' / 'rf_feature_importances.csv')\n",
    "except Exception as e:\n",
    "    print('Could not extract feature importances:', e)\n",
    "\n",
    "# %%\n",
    "\"\"\"## 21. Export: model, preprocessing objects and README\n",
    "Save a small README for using the trained model(s).\n",
    "\"\"\"\n",
    "readme = PROJECT_ROOT / 'models' / 'README.md'\n",
    "readme.write_text('# Trained models\n",
    "\n",
    "Pipelines saved: logistic_pipeline.joblib, rf_pipeline.joblib, lgb_pipeline.joblib (if present)\n",
    "\n",
    "To load and predict:\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "pipe = joblib.load(\"rf_pipeline.joblib\")\n",
    "proba = pipe.predict_proba(X_new)[:,1]\n",
    "```')\n",
    "\n",
    "print('Wrote model README at', readme)\n",
    "\n",
    "# %%\n",
    "\"\"\"## 22. Next automation steps (suggested)\n",
    "- Add this notebook/script to CI and run profiling as a nightly job (save artifacts to storage).\n",
    "- Add DVC to version datasets and models.\n",
    "- Wrap the best pipeline in a FastAPI service endpoint for production scoring.\n",
    "- Create a small dashboard for campaign uplift and decile analysis to prioritize prospects.\n",
    "\n",
    "End of extended notebook additions.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58a6fa-fe10-4569-9afa-d5ce5390e3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bankMenv",
   "language": "python",
   "name": "bankmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
